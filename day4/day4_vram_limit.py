import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import gc

def get_vram_usage():
    return torch.cuda.memory_allocated() / (1024 ** 2)

def main():
    if not torch.cuda.is_available():
        print("CUDA needed for VRAM test.")
        return

    model_id = "HuggingFaceTB/SmolLM2-1.7B-Instruct"
    print(f"Loading {model_id} for limit test...")
    
    # 4-bit ë¡œë“œë¡œ ìµœëŒ€í•œ ë§ì€ ë°°ì¹˜ í…ŒìŠ¤íŠ¸
    model = AutoModelForCausalLM.from_pretrained(
        model_id, 
        device_map="auto",
        torch_dtype=torch.float16
    )
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    
    batch_size = 1
    seq_len = 1024 # ê³ ì • ê¸¸ì´
    
    try:
        while True:
            torch.cuda.empty_cache()
            gc.collect()
            
            # ë”ë¯¸ ë°ì´í„° ìƒì„±
            input_ids = torch.randint(0, 1000, (batch_size, seq_len)).cuda()
            
            with torch.no_grad():
                _ = model(input_ids)
            
            print(f"Batch {batch_size} Success | VRAM: {get_vram_usage():.2f} MB")
            batch_size += 1 # 1ì”© ì¦ê°€ (ì •ë°€ ì¸¡ì •)
            
    except RuntimeError as e:
        if "out of memory" in str(e):
            print(f"\n[OOM Reached] Max Batch Size: {batch_size - 1}")
            print(f"Final VRAM: {get_vram_usage():.2f} MB")
        else:
            print(f"Error: {e}")

if __name__ == "__main__":
    main()


"""
Batch Size	VRAM ì‚¬ìš©ëŸ‰	ìƒíƒœ
Batch 1	3560 MB	âœ… ì„±ê³µ
Batch 5	4712 MB	âœ… ì„±ê³µ
Batch 10	6152 MB	âœ… ì„±ê³µ
Batch 15	7592 MB	âœ… ì„±ê³µ
Batch 17	8168 MB	âœ… ì„±ê³µ (ê±°ì˜ ê½‰ ì°¸)
Batch 18	8456 MB	âœ… ì„±ê³µ (SWAP/ê³µìœ  ë©”ëª¨ë¦¬ í™œìš© ì¶”ì •)
Batch 19	8744 MB	ğŸ›‘ ì„ê³„ì  ë„ë‹¬
"""